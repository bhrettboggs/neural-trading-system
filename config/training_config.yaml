# Training Configuration

training:
  # Optimizer settings
  optimizer: "adam"
  learning_rate: 0.001
  weight_decay: 0.0001
  
  # Learning rate scheduler
  scheduler:
    type: "reduce_on_plateau"
    mode: "min"
    factor: 0.5
    patience: 5
    min_lr: 0.00001
  
  # Training hyperparameters
  batch_size: 256
  epochs: 100
  early_stopping_patience: 10
  
  # Loss function
  loss: "binary_crossentropy"
  label_smoothing: 0.05  # Regularization: 0→0.05, 1→0.95
  class_weights: null  # Auto-balance if null
  
  # Gradient clipping
  gradient_clip: 1.0

# Data splits
data:
  # Example split - ADJUST THESE FOR YOUR ACTUAL DATA
  train_start: "2022-01-01"
  train_end: "2023-06-30"
  val_start: "2023-07-01"
  val_end: "2023-10-31"
  test_start: "2023-11-01"
  test_end: "2024-10-31"
  
  # Embargo between splits (bars)
  embargo_bars: 96  # 1 day for 15-min bars
  
  # Data source
  timeframe: "15m"
  symbol: "BTC/USDT"

# Cross-Validation
cross_validation:
  enabled: false  # Set to true for CV
  method: "purged_walk_forward"
  n_splits: 5
  embargo_bars: 96

# Hyperparameter Search (optional)
hyperparameter_search:
  enabled: false
  method: "optuna"  # Bayesian optimization
  n_trials: 50
  
  search_space:
    tcn_channels:
      - [32, 32, 64, 64]
      - [64, 64, 128, 128]
      - [16, 32, 64, 128]
    dropout: [0.1, 0.2, 0.3]
    learning_rate: [0.0001, 0.001, 0.01]
    batch_size: [128, 256, 512]
  
  # Optimization metric
  metric: "composite"  # composite = 0.4*sharpe + 0.3*(1-brier) + 0.3*auc

# Reproducibility
seed: 42

# Hardware
device: "cpu"  # Options: "cpu", "cuda", "mps"
num_workers: 0  # DataLoader workers

# Logging
logging:
  log_dir: "logs"
  tensorboard: false
  checkpoint_frequency: 5  # Save checkpoint every N epochs
  
# Model saving
model_saving:
  save_dir: "models"
  save_best_only: true
  monitor: "val_loss"
  mode: "min"